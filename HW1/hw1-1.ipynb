{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 Overview\n",
    "\n",
    "In this assignment, you will get familiar with basic document representation and analysis techniques. You will get the basic ideas of tokenization, stemming, normalization, constructing bag of words and TF-IDF representation for text documents.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set\n",
    "\n",
    "The instructor has prepared a small size collection of Yelp restaurant reviews (included in the provided sample Java project and separated into three folders) for this assignment . The review files are named and organized in the following manner:\n",
    "\n",
    "1. Each file contains all crawled review documents for a specific business on Yelp, and it is named by its unique ID on Yelp, e.g., *FAhx3UZtXvqNiEAd-GNruQ.json*;\n",
    "2. All the files are in json format. Each json file contains a json array of reviews ('Reviews') and a json object about the information of the restaurant ('RestaurantInfo').              \n",
    "\n",
    "\t2.1 The json object for **user review** is defined as follows:           \n",
    "\t\t{          \n",
    "\t\t\t'Author':'author name (string)',\n",
    "\t\t\t'ReviewID':'unique review id (string)',  \n",
    "\t\t\t'Overall':'numerical rating of the review (float)',\n",
    "\t\t\t'Content':'review text content (string)',   \n",
    "\t\t\t'Date':'date when the review was published',   \n",
    "\t\t\t'Author_Location':'author's registered location'  \n",
    "\t\t} \n",
    "    \n",
    "\t2.2 The json object for **restaurant info** is defined as follows:                                \n",
    "\t\t{                \n",
    "\t\t\t'RestaurantID':'unique business id in Yelp (string)',    \n",
    "\t\t\t'Name':'name of the business (string)',      \n",
    "\t\t\t'Price':'Yelp defined price range (string)',    \n",
    "\t\t\t'RestaurantURL':'actual URL to the business page on Yelp (string)',   \n",
    "\t\t\t'Longitude':'longitude of the business (double)',              \n",
    "\t\t\t'Latitude':'latitude of the business (double)',              \n",
    "\t\t\t'Address':'address of the business (string)',       \n",
    "\t\t\t'ImgURL':'URL to the business's image on Yelp (string)'     \n",
    "\t\t} \n",
    "\n",
    "In the following discussion, we will refer to each individual user review as a **document**. \n",
    "Note that some collected json files might not strictly follow the above json object definitions, e.g., some fields are missing or empty. As a result, properly handling the exceptions in json parsing is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Processing\n",
    "\n",
    "Apply NLTK library (http://www.nltk.org/install.html). Recall the following steps to pre-process the document: tokenization, normalization, stemming, stopwords removal.\n",
    "\n",
    "- Tokenization: tokenize the review content of each document into tokens.\n",
    "- Normalization: normalize the tokens from step 1, by removing individual punctuation marks (here is a list of [English punctuation marks](http://en.wikipedia.org/wiki/Punctuation_of_English)), converting tokens into lower cases, and recognizing digit numbers, e.g., integers and floats, and map them to a special symbol \"NUM\". \n",
    "- Stemming: stem the tokens back to their root form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understand Zipf's Law (50pts)\n",
    "\n",
    "First, let's validate the Zipf's law with the provided Yelp review data sets. This can be achieved by the following steps:\n",
    "\n",
    "1. Process the text document according to the discussed steps above.\n",
    "2. For each token, go over all the review documents containing it, and accumulate its frequency in those documents, i.e., total term frequency (TTF).\n",
    "3. Order the tokens by their TTF in a descending order.\n",
    "4. Create a dot plot by treating each word's rank as x-axis and its TTF as y-axis. Please use log-log scale for this plot.\n",
    "\n",
    "*Hint: basically, you can maintain a look-up table in memory while you are scanning through the documents, so that you only need to go through the corpus once to get the counts for all the tokens. In the provided implementation, this look-up table is maintained in the \"Corpus\" class, named as \"m_dictionary\".*\n",
    "\n",
    "From the resulting plot, can we find a strong linear relationship between the x and y axes in the log-log scale? If so, what is the slope of this linear relationship? To answer these questions, you can dump the data into excel and use the plot function there. (You can also use some scientific computing packages for curve fitting for this purpose.)          \n",
    "\n",
    "Then change the counting statistics in the previous experiment from *total term frequency (TTF)* to *document frequency (DF)* (i.e., how many documents contain this specific term), and perform the experiment again. Question: According to new curve and corresponding slope and intercept of the linear interpretation, can you conclude which counting statistics, i.e., *TTF* v.s., *DF*, fits Zipf's law better on this data set? Can you give any explanation about why it fits the law better?\n",
    "\n",
    "**What to submit**: \n",
    "\n",
    "1. Paste your implementation of text normalization module. (15pts)\n",
    "2. Two curves in log-log space generated above, with the corresponding slopes and intercepts of the linear interpretation results. (20pts)\n",
    "3. Your answers and thoughts to the above questions. (15pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieval with BoW and TF-IDF (50pts)\n",
    "\n",
    "Implement Bag-of-Word and TF-IDF query and document representations as retrieval models. For BoW, count the total number of documents that match with the 10 queries listed below accordingly,\n",
    "\n",
    "\tgeneral chicken\n",
    "\tfried chicken\n",
    "\tBBQ sandwiches\n",
    "\tmashed potatoes\n",
    "\tGrilled Shrimp Salad\n",
    "\tlamb Shank\n",
    "\tPepperoni pizza\n",
    "\tbrussel sprout salad\n",
    "\tFRIENDLY STAFF\n",
    "\tGrilled Cheese\n",
    "\n",
    "Please record the total running time and the number of returned documents accordingly.\n",
    "\n",
    "For TF-IDF, retrieve the top 3 documents for the 10 queries based on cosine similarity. You may use any library for TF-IDF such as [gensim TF-IDF](https://radimrehurek.com/gensim/models/tfidfmodel.html) or scikit-learn. \n",
    "\n",
    "**What to submit**: \n",
    "\n",
    "1. Paste your implementation of Bag-of-Word and TF-IDF representation (e.g., how to construct it from raw document content, and how to search for a particular query). (20pts)\n",
    "2. Running time and total number of returned documents by BoW (10pts)\n",
    "3. For the top 3 documents of each query returned by TF-IDF, print the documents and their cosine similarity scores. (10pts)\n",
    "4. For the first three queries, analyze the relation between relevance and cosine similarity score: is a high score document more relevant to the query? (10pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credits (5pts)\n",
    "\n",
    "You are encouraged to furthur investigate Zipf's law. For example (but not limited to), subsample the reviews with different sizes and see if an increased number of documents better help us verify Zipf's law by comparing the log-log curves.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "This assignment has in total 100 points. The deadline is May 9 23:59 PDT. You should submit your report in **PDF** using the homework latex template, and submit your code (notebook). To get a usable PDF notebook, first click on File > Print Preview, which will open in a new tab, then print to PDF using your browser's print functionality. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
